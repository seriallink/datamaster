package core

import (
	"bytes"
	"context"
	"embed"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/seriallink/datamaster/app/misc"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/service/ecs"
	"github.com/aws/aws-sdk-go-v2/service/ecs/types"
	"github.com/aws/aws-sdk-go-v2/service/emrserverless"
	emrtypes "github.com/aws/aws-sdk-go-v2/service/emrserverless/types"
	"github.com/aws/aws-sdk-go-v2/service/glue"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)

// BenchmarkResult holds detailed timing and resource usage information for a single benchmark run.
//
// This structure captures both the ECS task orchestration metadata (at the cluster level)
// and the actual execution metrics from within the container.
//
// Fields:
//
//   - Implementation: programming language used ("go" or "python").
//   - StartTime: timestamp when the benchmark command was triggered.
//   - EndTime: timestamp when the full benchmark flow completed (including ECS orchestration and result retrieval).
//   - StartClusterTime: timestamp when the ECS task was requested to start.
//   - EndClusterTime: timestamp when the ECS task reached the STOPPED status.
//   - StartTaskTime: timestamp at the beginning of the container's execution (inside the task).
//   - EndTaskTime: timestamp at the end of the container's execution (just before producing the result).
//   - InputFile: path to the input file (e.g., a `.csv.gz`) used in the benchmark.
//   - OutputFile: path to the output file (e.g., a `.parquet`) generated by the benchmark.
//   - CSVReadTime: time spent reading and parsing the input CSV.
//   - ParquetWriteTime: time spent writing the output to Parquet format.
//   - TotalTaskTime: total time elapsed inside the container (EndTaskTime - StartTaskTime).
//   - TotalBenchmarkTime: total time elapsed for the entire benchmark flow (EndTime - StartTime).
//   - MemoryUsedMB: memory usage observed inside the container (in megabytes).
//   - Notes: any additional contextual information (e.g., anomalies, comments).
type BenchmarkResult struct {
	Implementation     string        `json:"implementation,omitempty"`
	StartTime          time.Time     `json:"start_time,omitempty"`
	EndTime            time.Time     `json:"end_time,omitempty"`
	StartClusterTime   time.Time     `json:"start_cluster_time,omitempty"`
	EndClusterTime     time.Time     `json:"end_cluster_time,omitempty"`
	StartTaskTime      time.Time     `json:"start_task_time,omitempty"`
	EndTaskTime        time.Time     `json:"end_task_time,omitempty"`
	InputFile          string        `json:"input_file,omitempty"`
	OutputFile         string        `json:"output_file,omitempty"`
	CSVReadTime        time.Duration `json:"csv_read_time,omitempty"`
	ParquetWriteTime   time.Duration `json:"parquet_write_time,omitempty"`
	TotalTaskTime      time.Duration `json:"total_task_time,omitempty"`
	TotalBenchmarkTime time.Duration `json:"total_benchmark_time,omitempty"`
	MemoryUsedMB       float64       `json:"memory_used_mb,omitempty"`
	Notes              string        `json:"notes,omitempty"`
}

// RunEcsBenchmark executes a benchmark task in ECS using a Go-based container image.
//
// The function retrieves necessary stack outputs such as ECS cluster name,
// task definition, subnets, and security group from CloudFormation. It then
// runs a Fargate task configured with environment variables pointing to
// input, output, and result file paths in S3.
//
// The ECS task reads a `.csv.gz` file, writes the output as a Parquet file,
// and uploads a JSON result file with metrics. The function downloads this
// result, decodes it, and prints a benchmark report to stdout.
//
// Parameters:
//   - none
//
// Returns:
//   - error: an error if any step in the ECS task lifecycle or S3 download fails.
func RunEcsBenchmark() error {

	fmt.Println(misc.Blue("Launching Go benchmark (ECS Task)..."))

	cfg := GetAWSConfig()
	client := ecs.NewFromConfig(cfg)

	cluster, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkClusterName")
	if err != nil {
		return err
	}

	task, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkTaskDefinitionName")
	if err != nil {
		return err
	}

	subnets, err := (&Stack{Name: misc.StackNameNetwork}).GetStackOutput(cfg, "SubnetIds")
	if err != nil {
		return err
	}

	sg, err := (&Stack{Name: misc.StackNameSecurity}).GetStackOutput(cfg, "ProcessingSecurityGroup")
	if err != nil {
		return err
	}

	inputKey := "input/review.csv.gz"
	outputKey := "output/review-go.parquet"
	resultKey := "output/result-go.json"

	startCluster := time.Now()

	runOut, err := client.RunTask(context.TODO(), &ecs.RunTaskInput{
		Cluster:         aws.String(cluster),
		TaskDefinition:  aws.String(task),
		LaunchType:      types.LaunchTypeFargate,
		PlatformVersion: aws.String("LATEST"),
		NetworkConfiguration: &types.NetworkConfiguration{
			AwsvpcConfiguration: &types.AwsVpcConfiguration{
				Subnets:        strings.Split(subnets, ","),
				SecurityGroups: []string{sg},
				AssignPublicIp: types.AssignPublicIpEnabled,
			},
		},
		Overrides: &types.TaskOverride{
			ContainerOverrides: []types.ContainerOverride{
				{
					Name: aws.String("dm-benchmark-go-image"),
					Environment: []types.KeyValuePair{
						{Name: aws.String("BENCHMARK_INPUT"), Value: aws.String(inputKey)},
						{Name: aws.String("BENCHMARK_OUTPUT"), Value: aws.String(outputKey)},
						{Name: aws.String("BENCHMARK_RESULT"), Value: aws.String(resultKey)},
					},
				},
			},
		},
	})
	if err != nil {
		return fmt.Errorf("failed to run ECS task: %w", err)
	}

	if len(runOut.Tasks) == 0 {
		return fmt.Errorf("no ECS task started")
	}
	taskArn := *runOut.Tasks[0].TaskArn
	fmt.Println(misc.Blue(fmt.Sprintf("Task ARN: %s", taskArn)))

	waiter := ecs.NewTasksStoppedWaiter(client)
	err = waiter.Wait(context.TODO(), &ecs.DescribeTasksInput{
		Cluster: aws.String(cluster),
		Tasks:   []string{taskArn},
	}, 10*time.Minute)
	if err != nil {
		return fmt.Errorf("error waiting for ECS task to stop: %w", err)
	}
	endCluster := time.Now()

	bucket, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkBucketName")
	if err != nil {
		return err
	}

	obj, err := DownloadS3Object(context.TODO(), cfg, bucket, resultKey)
	if err != nil {
		return fmt.Errorf("failed to download benchmark result: %w", err)
	}
	defer obj.Body.Close()

	var result BenchmarkResult
	if err = json.NewDecoder(obj.Body).Decode(&result); err != nil {
		return fmt.Errorf("failed to decode result JSON: %w", err)
	}

	result.StartTime = startCluster
	result.EndTime = endCluster
	result.StartClusterTime = startCluster
	result.EndClusterTime = endCluster
	result.TotalBenchmarkTime = endCluster.Sub(startCluster)
	result.Notes = "ECS task completed successfully"

	PrintBenchmarkReport(result)
	return nil

}

// RunGlueBenchmark executes a benchmark using a PySpark Glue Job.
//
// This function launches a managed AWS Glue job that processes a `.csv.gz`
// input file from S3 and writes the result as a Parquet file. It also outputs
// a result JSON with benchmark metrics. The benchmark job is named
// "dm-benchmark-glue" and is parameterized with input, output, and result paths.
//
// After the job completes, the result JSON is downloaded, parsed, and printed
// as a report. The function measures the total elapsed time of the benchmark
// and includes it in the report.
//
// Parameters:
//   - none
//
// Returns:
//   - error: if the Glue job fails to start, complete, or the result cannot be downloaded or parsed.
func RunGlueBenchmark() error {

	cfg := GetAWSConfig()
	fmt.Println(misc.Blue("Launching PySpark benchmark (Glue Job)..."))

	bucket, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkBucketName")
	if err != nil {
		return err
	}

	jobName := "dm-benchmark-glue"

	inputKey := "input/review.csv.gz"
	outputKey := "output/review-glue/"
	resultKey := "output/result-glue.json"

	start := time.Now()

	glueClient := glue.NewFromConfig(cfg)
	runOutput, err := glueClient.StartJobRun(context.TODO(), &glue.StartJobRunInput{
		JobName: aws.String(jobName),
		Arguments: map[string]string{
			"--BENCHMARK_BUCKET": bucket,
			"--BENCHMARK_INPUT":  inputKey,
			"--BENCHMARK_OUTPUT": outputKey,
			"--BENCHMARK_RESULT": resultKey,
		},
	})
	if err != nil {
		return fmt.Errorf("failed to start Glue job: %w", err)
	}

	jobRunId := aws.ToString(runOutput.JobRunId)
	fmt.Println(misc.Blue("JobRunId: " + jobRunId))

	err = WaitForGlueJobFinished(context.TODO(), glueClient, jobName, jobRunId, 15*time.Minute)
	if err != nil {
		return fmt.Errorf("Glue job run failed: %w", err)
	}

	end := time.Now()

	obj, err := DownloadS3Object(context.TODO(), cfg, bucket, resultKey)
	if err != nil {
		return fmt.Errorf("failed to download benchmark result: %w", err)
	}
	defer obj.Body.Close()

	var resultData BenchmarkResult
	if err = json.NewDecoder(obj.Body).Decode(&resultData); err != nil {
		return fmt.Errorf("failed to parse result JSON: %w", err)
	}

	resultData.Implementation = "python"
	resultData.StartTime = start
	resultData.EndTime = end
	resultData.StartClusterTime = start
	resultData.EndClusterTime = end
	resultData.TotalBenchmarkTime = end.Sub(start)
	resultData.Notes = "Glue job completed successfully"

	PrintBenchmarkReport(resultData)
	return nil

}

// RunEmrBenchmark executes a PySpark benchmark using EMR Serverless.
//
// This function starts a job run on an existing EMR Serverless application,
// submitting a benchmark script located in S3 (`scripts/test.py`) and passing
// input/output/result paths as arguments. The script processes a `.csv.gz`
// file and outputs a Parquet file along with a JSON file containing
// performance metrics.
//
// After the job finishes, the result JSON is downloaded, decoded, and a
// benchmark report is printed to stdout. The total elapsed time is also measured
// and included in the report.
//
// Parameters:
//   - none
//
// Returns:
//   - error: if the job fails to start, complete, or if the result cannot be downloaded or parsed.
func RunEmrBenchmark() error {

	cfg := GetAWSConfig()
	fmt.Println(misc.Blue("Launching PySpark benchmark (EMR Serverless)..."))

	bucket, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkBucketName")
	if err != nil {
		return err
	}

	appID, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkAppId")
	if err != nil {
		return err
	}

	roleArn, err := (&Stack{Name: misc.StackNameRoles}).GetStackOutput(cfg, "EmrServerlessExecutionRoleArn")
	if err != nil {
		return err
	}

	inputKey := "input/review.csv.gz"
	outputKey := "benchmark/review/"
	resultKey := "output/result-emr.json"

	startCluster := time.Now()

	client := emrserverless.NewFromConfig(cfg)
	resp, err := client.StartJobRun(context.TODO(), &emrserverless.StartJobRunInput{
		ApplicationId:    aws.String(appID),
		ExecutionRoleArn: aws.String(roleArn),
		JobDriver: &emrtypes.JobDriverMemberSparkSubmit{
			Value: emrtypes.SparkSubmit{
				EntryPoint:            aws.String(fmt.Sprintf("s3://%s/scripts/test.py", bucket)),
				SparkSubmitParameters: nil,
				EntryPointArguments: []string{
					"--benchmark_bucket", bucket,
					"--benchmark_input", inputKey,
					"--benchmark_output", outputKey,
					"--benchmark_result", resultKey,
				},
			},
		},
		ConfigurationOverrides: &emrtypes.ConfigurationOverrides{
			MonitoringConfiguration: &emrtypes.MonitoringConfiguration{
				S3MonitoringConfiguration: &emrtypes.S3MonitoringConfiguration{
					LogUri: aws.String(fmt.Sprintf("s3://%s/logs/", bucket)),
				},
			},
		},
	})
	if err != nil {
		return fmt.Errorf("failed to submit EMR job: %w", err)
	}

	jobID := aws.ToString(resp.JobRunId)
	fmt.Println(misc.Blue("JobRunId: " + jobID))

	err = WaitForEMRJobFinished(context.Background(), client, appID, jobID, 15*time.Minute)
	if err != nil {
		return fmt.Errorf("job run failed: %w", err)
	}
	endCluster := time.Now()

	obj, err := DownloadS3Object(context.TODO(), cfg, bucket, resultKey)
	if err != nil {
		return fmt.Errorf("failed to download benchmark result: %w", err)
	}
	defer obj.Body.Close()
	if err != nil {
		return fmt.Errorf("failed to get result file from S3: %w", err)
	}
	defer obj.Body.Close()

	var resultData BenchmarkResult
	if err = json.NewDecoder(obj.Body).Decode(&resultData); err != nil {
		return fmt.Errorf("failed to parse result JSON: %w", err)
	}

	resultData.Implementation = "EMR (PySpark)"
	resultData.StartTime = startCluster
	resultData.EndTime = endCluster
	resultData.StartClusterTime = startCluster
	resultData.EndClusterTime = endCluster
	resultData.TotalBenchmarkTime = endCluster.Sub(startCluster)
	resultData.Notes = "EMR Serverless job completed successfully"

	PrintBenchmarkReport(resultData)
	return nil

}

// PrintBenchmarkReport displays the results of a benchmark execution.
//
// This function prints a formatted report with key metrics such as
// implementation type, orchestration time, task execution time, file paths,
// and performance statistics (e.g., CSV read time, Parquet write time,
// memory usage). It gracefully handles missing fields and only prints
// populated values.
//
// The function is intended to be called after decoding a BenchmarkResult,
// typically produced by an ECS task, Glue Job, or EMR Serverless job.
//
// Parameters:
//   - result: BenchmarkResult containing all relevant benchmark metrics.
//
// Returns:
//   - none
func PrintBenchmarkReport(result BenchmarkResult) {
	fmt.Println(misc.Yellow("=== Benchmark Result ==="))

	fmt.Printf("Implementation      : %s\n", result.Implementation)

	// Cluster-level orchestration
	if !result.StartClusterTime.IsZero() && !result.EndClusterTime.IsZero() {
		fmt.Printf("Start (Environment) : %s\n", result.StartClusterTime.Format(time.RFC3339))
		fmt.Printf("End   (Environment) : %s\n", result.EndClusterTime.Format(time.RFC3339))
		fmt.Printf("Orchestration Time  : %s\n", result.TotalBenchmarkTime)
	}

	// Task-level execution
	if !result.StartTaskTime.IsZero() && !result.EndTaskTime.IsZero() {
		fmt.Printf("Start (Task)        : %s\n", result.StartTaskTime.Format(time.RFC3339))
		fmt.Printf("End   (Task)        : %s\n", result.EndTaskTime.Format(time.RFC3339))
		fmt.Printf("Task Duration       : %s\n", result.TotalTaskTime)
	}

	// File paths
	if result.InputFile != "" {
		fmt.Printf("Input File          : %s\n", result.InputFile)
	}
	if result.OutputFile != "" {
		fmt.Printf("Output File         : %s\n", result.OutputFile)
	}

	// Performance metrics
	if result.CSVReadTime > 0 {
		fmt.Printf("CSV Read Time       : %s\n", result.CSVReadTime)
	}
	if result.ParquetWriteTime > 0 {
		fmt.Printf("Parquet Write Time  : %s\n", result.ParquetWriteTime)
	}
	if result.MemoryUsedMB > 0 {
		fmt.Printf("Memory Used (MB)    : %.2f\n", result.MemoryUsedMB)
	}

	// Notes
	if result.Notes != "" {
		fmt.Printf("Notes               : %s\n", result.Notes)
	}

	fmt.Println(misc.Yellow("========================\n"))

}

// WaitForEMRJobFinished polls the status of an EMR Serverless job until it completes or a timeout occurs.
//
// Parameters:
//   - ctx: context used for cancellation and timeouts.
//   - client: an instance of the EMR Serverless client.
//   - appID: the EMR Serverless application ID.
//   - jobID: the ID of the job run to monitor.
//   - timeout: the maximum duration to wait for job completion.
//
// Returns:
//   - error: nil if the job succeeded, or an error if it failed, was cancelled, or timed out.
func WaitForEMRJobFinished(ctx context.Context, client *emrserverless.Client, appID, jobID string, timeout time.Duration) error {

	start := time.Now()

	for {
		if time.Since(start) > timeout {
			return fmt.Errorf("timeout waiting for EMR job %s to complete", jobID)
		}

		resp, err := client.GetJobRun(ctx, &emrserverless.GetJobRunInput{
			ApplicationId: aws.String(appID),
			JobRunId:      aws.String(jobID),
		})
		if err != nil {
			return fmt.Errorf("failed to get EMR job status: %w", err)
		}

		state := string(resp.JobRun.State)
		//fmt.Printf("Job %s status: %s\n", jobID, state)

		switch state {
		case "SUCCESS":
			fmt.Printf("Job %s completed successfully\n", jobID)
			return nil
		case "FAILED", "CANCELLED", "CANCELLING":
			return fmt.Errorf("EMR job %s failed with state: %s", jobID, state)
		default:
			time.Sleep(10 * time.Second)
		}
	}

}

// WaitForGlueJobFinished waits until a Glue job run completes or the timeout is reached.
//
// It polls the Glue job status periodically (every few seconds) and checks for
// terminal states: SUCCEEDED, FAILED, TIMEOUT, or STOPPED. If the job completes
// successfully, it returns nil. If the job ends in a failure state or the
// timeout is exceeded, it returns an error.
//
// Parameters:
//   - ctx: context used to cancel or time out the polling loop.
//   - client: AWS Glue client used to query job status.
//   - jobName: name of the Glue job being monitored.
//   - runID: ID of the specific Glue job run.
//   - timeout: maximum duration to wait before giving up.
//
// Returns:
//   - error: if the job fails, is stopped, or exceeds the timeout duration.
func WaitForGlueJobFinished(ctx context.Context, client *glue.Client, jobName, runID string, timeout time.Duration) error {

	start := time.Now()
	for {
		if time.Since(start) > timeout {
			return fmt.Errorf("timed out waiting for Glue job to finish")
		}

		out, err := client.GetJobRun(ctx, &glue.GetJobRunInput{
			JobName:              aws.String(jobName),
			RunId:                aws.String(runID),
			PredecessorsIncluded: false,
		})
		if err != nil {
			return fmt.Errorf("failed to get job run status: %w", err)
		}

		state := string(out.JobRun.JobRunState)
		//fmt.Println(misc.Blue("JobRunState: " + state))

		switch state {
		case "SUCCEEDED":
			return nil
		case "FAILED", "TIMEOUT", "STOPPED":
			return fmt.Errorf("job failed with status: %s", state)
		case "STARTING", "RUNNING", "STOPPING":
			time.Sleep(5 * time.Second)
			continue
		default:
			time.Sleep(3 * time.Second)
		}
	}

}

// RunBenchmarkPostDeployment performs post-deployment setup for the benchmark environment.
//
// This function uploads the required benchmark input file and the Python scripts
// (`emr.py` and `glue.py`) to the appropriate S3 bucket. It assumes that the
// infrastructure stack has already been deployed and that the bucket output is available.
//
// Parameters:
//   - cfg: AWS configuration used to access S3.
//   - scripts: embedded file system containing the Python scripts to be uploaded.
//
// Returns:
//   - error: if any step (retrieving the bucket, uploading input or scripts) fails.
func RunBenchmarkPostDeployment(cfg aws.Config, scripts embed.FS) error {

	bucket, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkBucketName")
	if err != nil {
		return err
	}

	err = UploadBenchmarkInput()
	if err != nil {
		return err
	}

	err = UploadPythonScripts(cfg, scripts, bucket, "emr.py", "glue.py")
	if err != nil {
		return fmt.Errorf("failed to upload python scripts: %w", err)
	}

	return nil

}

// UploadBenchmarkInput downloads the benchmark CSV file and uploads it to the benchmark S3 bucket.
//
// This function fetches the file `review.csv.gz` from a public GitHub repository
// and uploads it to the `input/` prefix of the S3 bucket defined in the benchmark
// CloudFormation stack. It ensures the file is downloaded successfully, fully read
// into memory, and then stored in S3.
//
// Parameters:
//   - none
//
// Returns:
//   - error: if the file download, read, or upload to S3 fails.
func UploadBenchmarkInput() error {

	cfg := GetAWSConfig()
	client := s3.NewFromConfig(cfg)

	url := "https://raw.githubusercontent.com/seriallink/beer-datasets/main/review.csv.gz"

	resp, err := http.Get(url)
	if err != nil {
		return fmt.Errorf("failed to download: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("received status %s from GitHub", resp.Status)
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return fmt.Errorf("failed to read response body: %w", err)
	}

	bucket, err := (&Stack{Name: misc.StackNameBenchmark}).GetStackOutput(cfg, "BenchmarkBucketName")
	if err != nil {
		return err
	}

	key := "input/review.csv.gz"

	_, err = client.PutObject(context.TODO(), &s3.PutObjectInput{
		Bucket:        aws.String(bucket),
		Key:           aws.String(key),
		Body:          bytes.NewReader(body),
		ContentLength: aws.Int64(int64(len(body))),
	})
	if err != nil {
		return fmt.Errorf("failed to upload to S3: %w", err)
	}

	fmt.Printf("Uploaded benchmark file to s3://%s/%s\n", bucket, key)
	return nil

}
